---
title: "Datapalooza Report"
output:
  html_document:
    df_print: paged
---

Event Title: Datapalooza
Date: Friday, November 9th, 2018 
Time: 8am - 6pm 

Name: Erin Gunduz

*Main Summary*: Datapalooza 2018 was hosted by the University of Virginia Data Science Institute in coordination with UVA's Office of the Vice President for Research and Office of the Vice Present for Information Technology. Datapalooza was split up into several seminars throughout the day, featuring new areas of data science research across several disciplines and industries, like healthcare, business and technology. There were several presenters from various backgrounds. Below, I will give a summary of each seminar that took part throughout the day. 

  *Fireside Chat*: The day of Datapalooza began with a Fireside Chat with Phil Bourne, Stephenson Chair of Data Science, Director of Data Science Institute and Professor in the department of BME at UVA, and Jim Ryan, the 9th President of UVA. Datapalooza was introduced with a very powerful quote, "unprecedented data in the 21st century has the power to change the world". Arlyn Burgess, DSI Associate Director for Operations and Strategic Initiatives introduced Philip (Phil) Bourne. He describe that Datapalooza celebrates the current data revolution that is going on. The history of data science has evolved through four paradigms. The first paradigm involved making observations, followed by the second which focused on how we can use mathematics to model our environment. The third paradigm consisted of computation while the fourth and final paradigm is primarily data driven. One of Bourne's close friends, Tim Berners-Lee, considerably one of the fathers of the world wide web, stated that he could have never expected the exponential growth of data science that has taken over the world today. After this brief introduction, Jim Ryan came out and had an informal discussion with Phil about the intersection of data science with higher education at UVA. In essence, data science is a means to cut through the hierarchical structure of the institution of schooling because the attraction of data science is that it is interdisciplinary. Jim discussed that that one of the biggest challenges of higher education is whether there is sufficient organization and data sciences can help create these organized connections through cross-university collaborations. Furthermore, data science helps to mine data and ask the right questions, which is very important in observing the development of education. 
    
  The fireside chat concluded with a question and answer portion. Firstly, an individual in the audience asked what are the biggest challenges of data science in terms of ethical factors of data collection and use? This was answered by Jim Ryan who stated that the biggest challenge is privacy and ethics is an issue that is embedded into every course. The key aspect of it is to raise awareness of it. In such, we must first master vigilance in terms of understanding what are the ethical implications in the project etc. that you are doing. Furthermore, while we are reliant on policy making and legislation for privacy, those facets are local, yet data science is global phenomenon, which is why the privacy of data science can be very challenging. The second interesting question that was asked was about the role of data libraries in the future. They noted that there are papers published roughly every 3 seconds but half of these are never read. Nowadays, there is a lot more emphasis being put on the outcome of the data rather than the raw data itself. They concluded with the theory that subscription to print journals is unsustainable for the future and there are so many other means of disseminating information that should be utilized instead of just print journals. Finally, the last question that was asked in the fireside chat was what does the data-driven future look like for undergraduates? Jim Ryan answered this question by saying that Universities are generally doing more data science and less of everything else. He compared this to when PowerPoint originally came out, everyone was expected to be a graphic designer. Similarly, but on a much larger scale, with the growth of data science, universities will eventually require some literacy in data science. Essentially, he concluded that all undergraduates should have some competency in data science because it fundamentally trains us in the ability to ask the right questions during research, investigations and the learning process. 

  *Welcome & Introduction*: The Welcome & Introduction section of Datapalooza was presented by Arlyn Burgess. Datapalooza 2018 is the 4th iteration of this event. She depicted that the mission of Datapalooza and data science in general is to achieve excellence in data-driven research and education through solving important problems and providing for the workforce of tomorrow. Again, as mentioned in the fireside chat, data science revolves around teaching individuals how to ask the right questions. She concluded by mentioning how data science is a team sport and introduced the day-long activity of a connection web. Every individual who participated in Datapalooza had a specific ID number and every time you met someone and "made a connection" you could enter in their ID number into a software. This growing connection web would show the number of different individuals, from different backgrounds (undergraduate, graduate and professionals etc.) and how interconnected the topic of data science is. I thought this was a very interesting and appealing method to demonstrate the dense inter-connectivity of data science. 

  *Research Highlights (Machine Learning)*: The following section was Research Highlights. I specifically chose the section on machine learning because I thought it would be very interesting and I wanted to sit in a topic that was different than what I would hear during my Skills Session (data visualization). There were four separate presentations during research highlights. I will summarize all four below:
  1. The first presentation was titled "OpenEnsembles: A Python Toolkit for Ensemble Clustering", presented by Kristen Naegle. She began by defining clustering, which is the unsupervised learning techniques used to identify patterns (data dimensionality technique), and ensemble clustering, which is clustering more than one way while making perturbations each time. She also described that a distance metric is when groups are separated based on dissimilarities and clusters are grouped based on similarity. In other words, it's the method of gauging similarity and dissimilarity. Nevertheless, a challenge with this clustering algorithm is determining when to stop putting things in the same group and to start putting them in different groups, i.e. when to draw the line separating similar and dissimilar clusters. An ensemble also allows perturbations to be created across dimensions. However, clustering and algorithms are poor in ultra high dimensions (20D and up), but this can be bypassed by randomly creating sub-features in lower dimensions or by changing the number of clusters in the algorithm. She concluded by discussing how the emergent property of open clusters is that it focuses on connectivity rather than compactness. 
  2. The second presentation was about the application of data science in medicine, presented by Yekaterina Gilbo. The medical context that data science was applied to in this context was hypertrophic cardiomyopathy (HCM). HCM is the most common monogenic heart disease and the most frequent cause of sudden cardiac death in young adults and athletes in the United States. However, it is difficult to assess the risk of a patient to get HCM. Currently, medical treatments of HCM do not protect against cardiac death or heart failure. Data science is implemented into this application by using it to identify markers associated with HCM complications and risks for prognosis and therapeutic targets. Essentially, it creates an autonomous MRI toolbox for quantification and classification of areas like genetic testing and biomarker analysis.The HCM classification occurs into 6 different subtypes. The overall goal is to have machine learning analysis of biomarkers: risk stratification and therapeutic guidance. They used the software Keras library in Tensorflow to help achieve this goal. 
  3. The third presentation was titled "Plagiarism Detection using Semantic Analysis", presented by Samarth Singh, who is a first year graduate student in Engineering. He began by defining that plagiarism can be defined simply as the theft of intellectual property (IP). Essentially, it is the incorporation of someone else's work without providing adequate credit and this is becoming increasingly easier as the augmented availability of digital documents on the internet. Data science has been incorporated into this application by creating a software for detecting plagiarism. Older methods of plagiarism detection methods like word-to-word matching are no longer sufficient because plagiarizing techniques are becoming increasingly smarter by methods of changing active to passive verbs/tone and the use of adjectives etc. The software that was developed using data science machine learning for plagiarism detection uses various detection methods such as basic string, substring and subsequence matching (word-to-word matching), bags of word model, context based grammar, backend clustering for faster retrieval, and semantic methods applied on keywords (based on keywords). To briefly explain each method, bags of word model is encompassed around the fact that every sentence is represented in the form of vectors and this model finds cosine similarities between vectors. However, a major drawback of this method gives us no context whatsoever. Context based grammar is quite basic and it explains a set of rules that explains grammar similarities (it is good because it does provide context). Lastly, backend clusters take certain, similar key words out of the document, cluster and classify them. This is then compared to other clusters in other documents. He described that the overall project design conformed to the following three step process: extract features from document using topic modeling, clustering/classification of documents according to extracted features and finally, checking semantic similarities between any two given documents. The last step uses a similarity measure that is a fraction from 0 to 1, 1 being the highest. I thought a really cool aspect of the software was that it could do object-action pairing. For example, the Samarth gave an example of a sentence "Mike teaches rock-climbing after the probability class. He practices it in the weekend. He and Will also play soccer sometimes" and the software would be able to match object and actions into variables like: Mike: [teach, rock-climbing, probability, class, practice, soccer] and Will: [play, soccer]. This would aid in plagiarism detection. The efficiency of the algorithm was tested on a data set with varying amounts of plagiarism and it gave great results. There are some drawbacks to this algorithm. Firstly, there is a low level of success when the considered data set is small. Additionally, cross-language plagiarism detection is still lacking. Lastly, the software still doesn't look at intrinsic plagiarism, for example, how the tone changes throughout the paper, but the presenter acknowledged that this could be a future pathway for the algorithm.
  4. The last presentation of the research highlights section was titled "DeepRacing AI: Teaching autonomous vehicles to handle edge cases in traffic", presented by Madhur Behl who is part of the Cyber-Physical Systems Link Lab at the UVA. He began the presentation by discussing how machine intelligence is largely about training data. One of the largest issues is on how to correctly classify data. For example, an actual case of a Tesla with motion sensors showed that it can think a large bus is right next to the car, when it fact it is a washing machine and it is parked in a garage. This was a great example that transitioned to the overall question of how can we ensure that an autonomous vehicle remains safe in a situation that it has never encountered before? Some aspects that is important to consider is localization and mapping (where am I?), scene understanding (where and what is everything else?), trajectory planning and control (where do I go next and how can I steer/accelerate?) and lastly, human interaction (how can I convey my intent to the passenger and others around me?). A lot of the testing was done with formula 1 racers and cars to use data science to teach cars how to learn unknown tracks. They were able to create spatial-temporal maps of everything around the object and how it is all interconnected and had real-time data logging during experimentation. Essentially, they are working on a model that will allow a car to drive autonomously and reacting to unknown stimuli. For example, it can be applied to autonomous cars to detect things around you to increase safety, such as with drunk drivers. Their model was tested with un-annotated videos of race care drivers and in fact, the model was quite accurate in predicting and matching how an actual F1 driver drives on a course. One of the biggest challenges they are working with is how to label unlabeled videos (such as those that are used as a test for the model, e.g. F1 driver). They believe that they will be able to generate soft labels feeded to spatial-temporal networks because camera positions are the same in most cases so these can be used as pointers to see how it moves in reference to the frame. 

  *Skills Session (Data Visualization)*: After lunch, the second half of Datapalooza began with various skills sessions. I attended the session on Data Visualization, which was run by Steven Lee, CEO of Rotunda Solutions and Chip George, VP of Public Sector Alteryx. The skills session was titled "Using Alteryx: The thrill of solving" and it essentially discussed this novel software, Alteryx, that can be used for data visualization. Data visualization and analytics go hand in hand and Alteryx is a software that helps put data in a format that we want, in order to help show trends and patterns. All of these analytic problems begin with a question. They continued the presentation with some important figures: 40% of analysts time is spent getting the right data and 60% of these data sets have multiple inputs (not in the same format). Therefore, a large problem in data science is repeatability and that is one of the main goals of Alteryx. Additionally, almost 90% of data science models are only partially deployed, showing wasted efforts, which demonstrate that models are not as repeatable as we would like. Effectively, modeling inefficiencies account to roughly $60 billion. Therefore, visualization plays a big role in repeatability with data preparation and blending (cleaning of data), description (generating a report), spatialization (mapping) and predictions (what is going to happen next). A powerful quote that the presenters stated was that a good analyst shines when they tell the story behind the story and make deeper connections than just the obvious ones. Moving on, they began to discuss the features of the software Alteryx. Alteryx is a analytic tool that was the Gold Winner in 2017 Customer Choice Awards. It can run locally (on a desktop) and is quite easy to use. Effectively, it follows a workflow: data goes in, is manipulated and then data goes out following a process of importing data, data preparation, data modeling, predictive analysis, culminating in data output. I played around a little bit with the software during the presentation by using the available free trial. The configuration window is code-free and is located on the left of the screen. This part allows you to pictorially view your data, which makes it easier to identify if there is something wrong with the manipulation of data. The results window shows you what is happening to your data and is at the bottom of the screen. The parse tool separates out data, for example if data is all in one cell, with the use of parsing, the data would be separated into individual cells. Furthermore, data cleansing is a tool used to clean up the data. A large benefit of this software is that you can look at the data right in workflow and you need minimal competency of coding skills. Visualization is critical because data gathering and manipulation is not worth anything if it can't be accurately visualized and represented. They stated that the most attractive aspect of this software is that it is an equalizer in the sense that even if you have never written script in your life, playing around with it for a couple weeks makes you as good as an analyst in terms of visualization because you could essentially produce everything that they could. It is a tool for everyone to use including people that are either inexperienced or experienced because it ultimately opens up a lot of doors that can help coders visualize data much quicker and easier. They concluded by saying that it is a super tool for teaching data visualization, especially when compared to R or Python script because in those cases you don't get a lot of the context. 
  Fun Fact: I met an analyst who works for the Fire Department in Charlottesville during the skills session. He is an alumni of UVA, lives on a farm with a lot of chickens and has a poodle named Yoshi. 

  *Keynote*: The final event of the day was the Keynote speech given by Robin Thottungal, Chief Technology Officer/ and Chief Data Scientist at the National Gallery of Art. Overall, Robin discussed the implementation of artificial intelligence (AI) in the museum experience as it maximizes interactive experience and makes it more enjoyable. He began his presentation by introducing a news article, which was very surprising to me: "Most of the World's Best Places for Coffee will be Gone by 2050". As an avid coffee drinker I was surprised by this. The reason for this is attributed to climate change. Effectively, as temperature increases, the coffee bean belt (which is a belt of locations near the equator that are the most ideal places for cultivating coffee) is reducing. Climate changes leads to increasing global temperatures that lead to more fungi on the bean that prevents cultivation. Robin then transitioned his presentation to discuss the benefits of data science. Data science can help policy makers understand the complex interplay and data visualization can help greatly with this as it can be a means of driving transparency into complex issues. Essentially, it helps discussion makers actually see what is happening, to the environment in this example. Data scientists can help tell stories with the correct data and methods of visualization, so people know which questions are the right ones to answer so individuals can understand the full story of what is going on. Robin gave the example of how data science can use impact analysis to help first responders to natural disasters to make sure they are doing the right thing at the right time. Robin then moved on to discussing the potential dangers of using AI, which was the most interesting part of his presentation in my opinion. One of the major dangers of using AI was on the point of predictive policies. This is a real life example, which I found shocking. A few years ago, an algorithm was created that allowed law makers etc. to run pictures of individuals and the algorithm would generate a score of how likely that individual would be to commit a crime solely based off of their facial features. Evidently, there were so many problems with this approach and revealed many ethical issues. In the example that he showed us, the algorithm was run on two black people (a man and a woman) and two white males. Both black individuals had a higher score than the two white people (10/10 and 8/10, respectively compared to 2/10 and 3/10, respectively). However, in this case, actually both the white individuals committed more serious crimes on the level of murder and bank robbery. There are also cases of judges giving sentencing to individuals based on if they had a high score generated from this algorithm, incorrectly putting all their confidence into AI. This was incredibly shocking to me how this technology was used solely to determine the fate of certain individuals. It highlights the concept that data scientists need empathy the most. This example also brought up the concern of whether the technology itself is ethical/racist or is the individuals that are using it. It also brought to light a key component that we shouldn't use algorithms to make decisions but rather just to ask questions. Effectively, AI can affect people's lives and if there is a bias, then it could be very bad and it is our duty to be responsible. He concluded by briefly mentioning his job at the museum. He uses data science to map out the path of people when they are observing an exhibit, using heat spots to demonstrate how long people are staying at which sections of the exhibit. This can help curators to change the direction or flow of the gallery to maximize user interaction and interest and make the experience more  engaging. 
  Datapalooza ended with a reception. 

  *Conclusion and Final Thoughts*: Overall, I really enjoyed Datapalooza. I learned a lot about the multitude of applications of data science and I was also able to visualize the potential use of data science in the future. My favorite sections of the data was the discussion on the use of data science in plagiarism because I thought it was very interesting and his presentation was very clear and interesting to listen to. I also extremely enjoyed the keynote speech at the end, especially the discussion of the disadvantages of AI because I learned a lot of new information that was quite shocking and intriguing to me (especially about the racial profiling). I also enjoyed Robin's presentation because it was easy to follow and very engaging. All in all, I had a great day and met some interesting people while learning about the promising and exciting future of data science. 
  
  